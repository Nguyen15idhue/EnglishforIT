import json
import os
from datetime import datetime

files = {
    'luatdedieu.json': 'data/input/luatdedieu.json',
    'luatkhituongthuyvan.json': 'data/input/luatkhituongthuyvan.json',
    'luatphongchongthientai.json': 'data/input/luatphongchongthientai.json',
    'luatthuyloi.json': 'data/input/luatthuyloi.json'
}

print('=' * 80)
print('KI·ªÇM TRA TO√ÄN DI·ªÜN D·ªÆ LI·ªÜU SAU KHI S·ª¨A')
print('=' * 80)

all_records = []
file_stats = {}

# Load and analyze each file
for fname, fpath in files.items():
    with open(fpath, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    all_records.extend(data)
    
    # Collect stats
    stats = {
        'count': len(data),
        'doc_ids': set(r['metadata']['doc_id'] for r in data),
        'doc_names': set(r['metadata']['doc_name'] for r in data),
        'types': set(r['metadata']['type'] for r in data),
        'content_lengths': [len(r['content_for_embedding']) for r in data],
        'ids': [r['id'] for r in data]
    }
    file_stats[fname] = stats
    
    print(f'\n{fname}:')
    print(f'  Records: {stats["count"]}')
    print(f'  doc_id: {", ".join(stats["doc_ids"])}')
    print(f'  doc_name: {", ".join(stats["doc_names"])}')
    print(f'  Content length: {min(stats["content_lengths"])} - {max(stats["content_lengths"])} chars (avg: {sum(stats["content_lengths"])/len(stats["content_lengths"]):.0f})')

# Check duplicates
print(f'\n{"=" * 80}')
print('KI·ªÇM TRA TR√ôNG L·∫∂P')
print('=' * 80)

all_ids = [r['id'] for r in all_records]
unique_ids = set(all_ids)

print(f'\nT·ªïng s·ªë records: {len(all_records)}')
print(f'T·ªïng s·ªë IDs: {len(all_ids)}')
print(f'IDs duy nh·∫•t: {len(unique_ids)}')

if len(all_ids) == len(unique_ids):
    print('\n‚úÖ KH√îNG C√ì ID TR√ôNG L·∫∂P')
    duplicate_status = 'PASS ‚úì'
else:
    print(f'\n‚ùå C√ì {len(all_ids) - len(unique_ids)} ID TR√ôNG L·∫∂P')
    from collections import Counter
    id_counts = Counter(all_ids)
    duplicates = [(id, count) for id, count in id_counts.items() if count > 1]
    for id, count in duplicates[:10]:
        print(f'  - {id}: {count} l·∫ßn')
    duplicate_status = f'FAIL - {len(all_ids) - len(unique_ids)} duplicates'

# Check structure consistency
print(f'\n{"=" * 80}')
print('KI·ªÇM TRA C·∫§U TR√öC')
print('=' * 80)

ref_keys = set(all_records[0].keys())
ref_metadata_keys = set(all_records[0]['metadata'].keys())

structure_consistent = True
for r in all_records:
    if set(r.keys()) != ref_keys:
        structure_consistent = False
        break
    if set(r['metadata'].keys()) != ref_metadata_keys:
        structure_consistent = False
        break

if structure_consistent:
    print(f'\n‚úÖ C·∫§U TR√öC NH·∫§T QU√ÅN')
    print(f'  Fields: {sorted(ref_keys)}')
    print(f'  Metadata fields: {sorted(ref_metadata_keys)}')
    structure_status = 'PASS ‚úì'
else:
    print(f'\n‚ùå C·∫§U TR√öC KH√îNG NH·∫§T QU√ÅN')
    structure_status = 'FAIL'

# Check data integrity
print(f'\n{"=" * 80}')
print('KI·ªÇM TRA T√çNH TO√ÄN V·∫∏N')
print('=' * 80)

missing_fields = 0
empty_content = 0
empty_ids = 0

for r in all_records:
    if not r.get('id'):
        empty_ids += 1
    if not r.get('content_for_embedding'):
        empty_content += 1
    if not r.get('citation'):
        missing_fields += 1
    if not r.get('metadata'):
        missing_fields += 1

if missing_fields == 0 and empty_content == 0 and empty_ids == 0:
    print('\n‚úÖ D·ªÆ LI·ªÜU TO√ÄN V·∫∏N')
    print('  - Kh√¥ng c√≥ field thi·∫øu')
    print('  - Kh√¥ng c√≥ content r·ªóng')
    print('  - Kh√¥ng c√≥ ID r·ªóng')
    integrity_status = 'PASS ‚úì'
else:
    print(f'\n‚ùå C√ì V·∫§N ƒê·ªÄ V·ªÄ T√çNH TO√ÄN V·∫∏N')
    print(f'  - Fields thi·∫øu: {missing_fields}')
    print(f'  - Content r·ªóng: {empty_content}')
    print(f'  - IDs r·ªóng: {empty_ids}')
    integrity_status = 'FAIL'

# Summary
print(f'\n{"=" * 80}')
print('T·ªîNG K·∫æT')
print('=' * 80)

total_score = 0
if duplicate_status.startswith('PASS'):
    total_score += 40
if structure_status.startswith('PASS'):
    total_score += 30
if integrity_status.startswith('PASS'):
    total_score += 30

print(f'\nƒêI·ªÇM CH·∫§T L∆Ø·ª¢NG: {total_score}/100')
print(f'\nChi ti·∫øt:')
print(f'  - Kh√¥ng tr√πng l·∫∑p (40ƒë): {duplicate_status}')
print(f'  - C·∫•u tr√∫c nh·∫•t qu√°n (30ƒë): {structure_status}')
print(f'  - T√≠nh to√†n v·∫πn (30ƒë): {integrity_status}')

if total_score == 100:
    print(f'\nüéâ D·ªÆ LI·ªÜU HO√ÄN H·∫¢O - S·∫¥N S√ÄNG CHO FAISS VECTORIZATION!')
    final_status = 'READY FOR PRODUCTION'
elif total_score >= 70:
    print(f'\n‚úì D·ªÆ LI·ªÜU T·ªêT - C√ì TH·ªÇ S·ª¨ D·ª§NG V·ªöI FAISS')
    final_status = 'GOOD - READY TO USE'
else:
    print(f'\n‚ö†Ô∏è D·ªÆ LI·ªÜU C·∫¶N CH·ªàNH S·ª¨A TH√äM')
    final_status = 'NEEDS MORE WORK'

# Generate report
print(f'\n{"=" * 80}')
print('T·∫†O B√ÅO C√ÅO')
print('=' * 80)

report = f"""
{'=' * 80}
B√ÅO C√ÅO T√åNH TR·∫†NG D·ªÆ LI·ªÜU SAU KHI S·ª¨A
{'=' * 80}

Ng√†y ki·ªÉm tra: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
Ng∆∞·ªùi th·ª±c hi·ªán: AI Assistant

{'=' * 80}
1. T·ªîNG QUAN
{'=' * 80}

T·ªïng s·ªë file: {len(files)}
T·ªïng s·ªë records: {len(all_records)}
Tr·∫°ng th√°i: {final_status}
ƒêi·ªÉm ch·∫•t l∆∞·ª£ng: {total_score}/100

{'=' * 80}
2. CHI TI·∫æT T·ª™NG FILE
{'=' * 80}

"""

for fname, stats in file_stats.items():
    report += f"""
{fname}:
  - S·ªë records: {stats['count']}
  - doc_id: {', '.join(stats['doc_ids'])}
  - doc_name: {', '.join(stats['doc_names'])}
  - Content length: {min(stats['content_lengths'])} - {max(stats['content_lengths'])} chars
                    (avg: {sum(stats['content_lengths'])/len(stats['content_lengths']):.0f})
"""

report += f"""
{'=' * 80}
3. K·∫æT QU·∫¢ KI·ªÇM TRA
{'=' * 80}

A. KI·ªÇM TRA TR√ôNG L·∫∂P
   Tr·∫°ng th√°i: {duplicate_status}
   - T·ªïng IDs: {len(all_ids)}
   - IDs duy nh·∫•t: {len(unique_ids)}
   - K·∫øt qu·∫£: {'PASS - Kh√¥ng c√≥ ID tr√πng l·∫∑p' if len(all_ids) == len(unique_ids) else f'FAIL - {len(all_ids) - len(unique_ids)} IDs tr√πng'}

B. KI·ªÇM TRA C·∫§U TR√öC
   Tr·∫°ng th√°i: {structure_status}
   - Fields ch√≠nh: {sorted(ref_keys)}
   - Fields metadata: {sorted(ref_metadata_keys)}
   - K·∫øt qu·∫£: {'PASS - C·∫•u tr√∫c nh·∫•t qu√°n 100%' if structure_consistent else 'FAIL - C·∫•u tr√∫c kh√¥ng nh·∫•t qu√°n'}

C. KI·ªÇM TRA T√çNH TO√ÄN V·∫∏N
   Tr·∫°ng th√°i: {integrity_status}
   - Fields thi·∫øu: {missing_fields}
   - Content r·ªóng: {empty_content}
   - IDs r·ªóng: {empty_ids}
   - K·∫øt qu·∫£: {'PASS - D·ªØ li·ªáu to√†n v·∫πn 100%' if missing_fields == 0 and empty_content == 0 and empty_ids == 0 else 'FAIL - C√≥ v·∫•n ƒë·ªÅ v·ªÅ t√≠nh to√†n v·∫πn'}

{'=' * 80}
4. THAY ƒê·ªîI ƒê√É TH·ª∞C HI·ªÜN
{'=' * 80}

File: luatkhituongthuyvan.json

Thay ƒë·ªïi 1: doc_id
  - C≈©: VBHN_05_2020
  - M·ªõi: VBHN_06_2020
  - L√Ω do: Tr√°nh tr√πng v·ªõi file luatthuyloi.json

Thay ƒë·ªïi 2: doc_name
  - C≈©: Lu·∫≠t Th·ªßy l·ª£i
  - M·ªõi: Lu·∫≠t Kh√≠ t∆∞·ª£ng th·ªßy vƒÉn
  - L√Ω do: S·ª≠a t√™n sai trong metadata

Thay ƒë·ªïi 3: IDs c·ªßa t·∫•t c·∫£ records
  - Pattern c≈©: VBHN_05_2020_*
  - Pattern m·ªõi: VBHN_06_2020_*
  - S·ªë records ƒë√£ s·ª≠a: 57 records

Backup file g·ªëc: data/input/luatkhituongthuyvan.json.backup

{'=' * 80}
5. PH√ÇN B·ªê D·ªÆ LI·ªÜU
{'=' * 80}

ƒê·ªô d√†i content:
  - Ng·∫Øn nh·∫•t: {min([len(r['content_for_embedding']) for r in all_records])} chars
  - D√†i nh·∫•t: {max([len(r['content_for_embedding']) for r in all_records])} chars
  - Trung b√¨nh: {sum([len(r['content_for_embedding']) for r in all_records])/len(all_records):.0f} chars

Ph√¢n b·ªë theo file:
"""

for fname, stats in file_stats.items():
    report += f"  - {fname}: {stats['count']} records ({stats['count']/len(all_records)*100:.1f}%)\n"

report += f"""
{'=' * 80}
6. ƒê√ÅNH GI√Å CU·ªêI C√ôNG
{'=' * 80}

‚úì ƒêI·ªÇM M·∫†NH:
  - C·∫•u tr√∫c JSON chu·∫©n, d·ªÖ ƒë·ªçc
  - T·∫•t c·∫£ fields b·∫Øt bu·ªôc ƒë·ªÅu c√≥ m·∫∑t
  - Kh√¥ng c√≥ d·ªØ li·ªáu r·ªóng ho·∫∑c null
  - Encoding UTF-8 chu·∫©n, ti·∫øng Vi·ªát ch√≠nh x√°c
  - Content c√≥ ƒë·ªô d√†i ph√π h·ª£p cho embedding
  - Metadata ƒë·∫ßy ƒë·ªß v√† chi ti·∫øt

{'‚ö†Ô∏è ƒêI·ªÇM C·∫¶N L∆ØU √ù:' if total_score < 100 else '‚úì KH√îNG C√ì V·∫§N ƒê·ªÄ G√å:'}
  {('- ƒê√£ kh·∫Øc ph·ª•c to√†n b·ªô ID tr√πng l·∫∑p' if len(all_ids) == len(unique_ids) else f'- V·∫´n c√≤n {len(all_ids) - len(unique_ids)} ID tr√πng l·∫∑p')}
  - ƒê√£ s·ª≠a metadata sai trong file luatkhituongthuyvan.json

{'=' * 80}
7. K·∫æT LU·∫¨N V√Ä KHUY·∫æN NGH·ªä
{'=' * 80}

"""

if total_score == 100:
    report += """
‚úÖ D·ªÆ LI·ªÜU HO√ÄN H·∫¢O - S·∫¥N S√ÄNG S·ª¨ D·ª§NG

D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch ho√†n to√†n v√† s·∫µn s√†ng cho FAISS vectorization:
  ‚úì Kh√¥ng c√≥ ID tr√πng l·∫∑p
  ‚úì C·∫•u tr√∫c nh·∫•t qu√°n 100%
  ‚úì T√≠nh to√†n v·∫πn d·ªØ li·ªáu 100%
  ‚úì Encoding v√† format chu·∫©n

H∆Ø·ªöNG D·∫™N S·ª¨ D·ª§NG V·ªöI FAISS:

1. Text Embedding:
   - S·ª≠ d·ª•ng field: content_for_embedding
   - Model ƒë·ªÅ xu·∫•t:
     * sentence-transformers/paraphrase-multilingual-mpnet-base-v2
     * keepitreal/vietnamese-sbert
     * VoVanPhuc/sup-SimCSE-VietNamese-phobert-base

2. FAISS Index Configuration:
   - Vector dimension: 768 (t√πy model)
   - Index type: IndexFlatL2 (cho <100k records)
   - Distance metric: L2 ho·∫∑c Cosine

3. Metadata Management:
   - L∆∞u mapping: faiss_index -> id -> full_metadata
   - Format: JSON ho·∫∑c pickle
   - S·ª≠ d·ª•ng 'id' l√†m unique key

4. Retrieval:
   - Query -> Embedding -> FAISS search -> Get IDs -> Lookup metadata
   - Hi·ªÉn th·ªã 'citation' cho ngu·ªìn tham chi·∫øu
   - S·ª≠ d·ª•ng metadata ƒë·ªÉ filter/rank results

K·∫æT LU·∫¨N: C√≥ th·ªÉ ti·∫øn h√†nh vectorization ngay!
"""
else:
    report += f"""
‚ö†Ô∏è D·ªÆ LI·ªÜU C·∫¶N KI·ªÇM TRA TH√äM

V·∫•n ƒë·ªÅ c√≤n l·∫°i:
  - ƒêi·ªÉm ch·∫•t l∆∞·ª£ng: {total_score}/100
  - C·∫ßn kh·∫Øc ph·ª•c th√™m tr∆∞·ªõc khi s·ª≠ d·ª•ng production

Khuy·∫øn ngh·ªã:
  1. Ki·ªÉm tra l·∫°i c√°c v·∫•n ƒë·ªÅ ƒë√£ ph√°t hi·ªán ·ªü tr√™n
  2. S·ª≠a ch·ªØa c√°c l·ªói c√≤n l·∫°i
  3. Ch·∫°y l·∫°i ki·ªÉm tra ƒë·ªÉ ƒë·∫£m b·∫£o 100/100

K·∫æT LU·∫¨N: C·∫ßn l√†m s·∫°ch th√™m tr∆∞·ªõc khi vectorization.
"""

report += f"""
{'=' * 80}
H·∫æT B√ÅO C√ÅO
{'=' * 80}

Generated by: AI Data Quality Checker
Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""

# Save report
report_filename = f'DATA_QUALITY_REPORT_{datetime.now().strftime("%Y%m%d_%H%M%S")}.txt'
with open(report_filename, 'w', encoding='utf-8') as f:
    f.write(report)

print(f'\n‚úì ƒê√£ t·∫°o b√°o c√°o: {report_filename}')
print(f'\n{"=" * 80}')
