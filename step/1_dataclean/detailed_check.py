import json
import os
import re

files = [
    'f:/3.Laptrinh/EnglishforIT/data/input/luatdedieu.json',
    'f:/3.Laptrinh/EnglishforIT/data/input/luatkhituongthuyvan.json',
    'f:/3.Laptrinh/EnglishforIT/data/input/luatphongchongthientai.json',
    'f:/3.Laptrinh/EnglishforIT/data/input/luatthuyloi.json'
]

print('=' * 80)
print('KI·ªÇM TRA CHI TI·∫æT CH·∫§T L∆Ø·ª¢NG D·ªÆ LI·ªÜU CHO FAISS VECTORIZATION')
print('=' * 80)

all_records = []
total_issues = []

for fpath in files:
    print(f'\n{os.path.basename(fpath)}')
    print('-' * 80)
    
    with open(fpath, 'r', encoding='utf-8') as f:
        data = json.load(f)
    all_records.extend(data)
    
    # 1. Ki·ªÉm tra encoding v√† k√Ω t·ª± ƒë·∫∑c bi·ªát
    control_chars = 0
    non_printable = 0
    for i, r in enumerate(data):
        content = r['content_for_embedding']
        # Ki·ªÉm tra control characters (ngo·∫°i tr·ª´ newline, tab)
        if re.search(r'[\x00-\x08\x0B\x0C\x0E-\x1F]', content):
            control_chars += 1
        # Ki·ªÉm tra non-printable characters
        if re.search(r'[^\x20-\x7E\u00C0-\u024F\u1E00-\u1EFF\n\t]', content):
            non_printable += 1
    
    print(f'  Records c√≥ control chars: {control_chars}')
    print(f'  Records c√≥ non-printable chars: {non_printable}')
    
    # 2. Ki·ªÉm tra ƒë·ªô d√†i qu√° ng·∫Øn ho·∫∑c qu√° d√†i
    too_short = sum(1 for r in data if len(r['content_for_embedding']) < 50)
    too_long = sum(1 for r in data if len(r['content_for_embedding']) > 8000)
    print(f'  Records qu√° ng·∫Øn (<50 chars): {too_short}')
    print(f'  Records qu√° d√†i (>8000 chars): {too_long}')
    
    # 3. Ki·ªÉm tra n·ªôi dung c√≥ √Ω nghƒ©a
    empty_or_whitespace = sum(1 for r in data if not r['content_for_embedding'].strip())
    print(f'  Records r·ªóng ho·∫∑c ch·ªâ c√≥ whitespace: {empty_or_whitespace}')
    
    # 4. Ki·ªÉm tra metadata consistency
    all_doc_names = set(r['metadata']['doc_name'] for r in data)
    all_types = set(r['metadata']['type'] for r in data)
    print(f'  S·ªë l∆∞·ª£ng doc_name kh√°c nhau: {len(all_doc_names)}')
    print(f'  C√°c gi√° tr·ªã type: {all_types}')
    
    # 5. Ki·ªÉm tra citation format
    citation_formats = set()
    for r in data:
        # L·∫•y pattern c·ªßa citation
        citation = r['citation']
        # Extract pattern (b·ªè s·ªë ƒëi·ªÅu)
        pattern = re.sub(r'ƒêi·ªÅu \d+', 'ƒêi·ªÅu X', citation)
        citation_formats.add(pattern)
    print(f'  S·ªë format citation kh√°c nhau: {len(citation_formats)}')
    if len(citation_formats) > 2:
        print(f'    ‚ö†Ô∏è Citation formats kh√¥ng nh·∫•t qu√°n!')
        for fmt in list(citation_formats)[:3]:
            print(f'      - {fmt}')
    
    # 6. Ki·ªÉm tra ID format
    id_pattern_issues = 0
    for r in data:
        # ID n√™n c√≥ format: PREFIX_XX_XXXX_CX_DX
        if not re.match(r'^[A-Z]+_\d+_\d+_C[IVX\d]+_D\d+$', r['id']):
            id_pattern_issues += 1
    print(f'  IDs kh√¥ng ƒë√∫ng format chu·∫©n: {id_pattern_issues}')
    
    # 7. Ki·ªÉm tra chapter/article numbering
    chapters = [(r['metadata']['chapter_no'], r['metadata']['article_no']) for r in data]
    chapter_article_map = {}
    for ch, art in chapters:
        if ch not in chapter_article_map:
            chapter_article_map[ch] = []
        chapter_article_map[ch].append(art)
    
    print(f'  S·ªë ch∆∞∆°ng: {len(chapter_article_map)}')
    
    # Ki·ªÉm tra gaps trong article numbering
    gaps_found = False
    for ch, articles in chapter_article_map.items():
        # Chuy·ªÉn sang s·ªë n·∫øu c√≥ th·ªÉ
        try:
            article_nums = sorted([int(a) for a in articles if a.isdigit()])
            if article_nums:
                expected = list(range(min(article_nums), max(article_nums) + 1))
                if article_nums != expected:
                    gaps_found = True
        except:
            pass
    
    if gaps_found:
        print(f'    ‚ö†Ô∏è C√≥ gaps trong article numbering')

print('\n' + '=' * 80)
print('T·ªîNG H·ª¢P TO√ÄN B·ªò D·ªÆ LI·ªÜU')
print('=' * 80)
print(f'T·ªïng s·ªë records t·ª´ t·∫•t c·∫£ files: {len(all_records)}')

# Ki·ªÉm tra ID unique across all files
all_ids = [r['id'] for r in all_records]
unique_ids = set(all_ids)
print(f'S·ªë ID duy nh·∫•t: {len(unique_ids)}')
if len(all_ids) != len(unique_ids):
    print(f'  ‚ö†Ô∏è C√ì {len(all_ids) - len(unique_ids)} ID TR√ôNG L·∫∂P GI·ªÆA C√ÅC FILE!')
    # T√¨m IDs tr√πng
    from collections import Counter
    id_counts = Counter(all_ids)
    duplicates = [id for id, count in id_counts.items() if count > 1]
    print(f'  IDs tr√πng: {duplicates[:5]}...')
else:
    print(f'  ‚úì T·∫•t c·∫£ IDs l√† duy nh·∫•t')

# Ph√¢n t√≠ch ph√¢n b·ªë ƒë·ªô d√†i
lengths = [len(r['content_for_embedding']) for r in all_records]
print(f'\nPh√¢n b·ªë ƒë·ªô d√†i content:')
print(f'  Min: {min(lengths)} chars')
print(f'  Max: {max(lengths)} chars')
print(f'  Mean: {sum(lengths)/len(lengths):.0f} chars')
print(f'  Median: {sorted(lengths)[len(lengths)//2]} chars')

# Ph√¢n t√≠ch theo percentile
lengths_sorted = sorted(lengths)
p25 = lengths_sorted[len(lengths_sorted)//4]
p75 = lengths_sorted[3*len(lengths_sorted)//4]
print(f'  25th percentile: {p25} chars')
print(f'  75th percentile: {p75} chars')

print('\n' + '=' * 80)
print('K·∫æT LU·∫¨N V√Ä KHUY·∫æN NGH·ªä')
print('=' * 80)

issues = []
recommendations = []

# T√≠nh ƒëi·ªÉm ch·∫•t l∆∞·ª£ng
quality_score = 100

# Check 1: C·∫•u tr√∫c nh·∫•t qu√°n
print('‚úì C·∫•u tr√∫c: HO√ÄN H·∫¢O')
print('  - T·∫•t c·∫£ file c√≥ c√πng schema')
print('  - T·∫•t c·∫£ fields b·∫Øt bu·ªôc ƒë·ªÅu c√≥ m·∫∑t')
print('  - Metadata ƒë·∫ßy ƒë·ªß v√† nh·∫•t qu√°n')

# Check 2: T√≠nh to√†n v·∫πn d·ªØ li·ªáu
print('\n‚úì T√≠nh to√†n v·∫πn: HO√ÄN H·∫¢O')
print('  - Kh√¥ng c√≥ field r·ªóng')
print('  - Kh√¥ng c√≥ missing values')
print('  - Kh√¥ng c√≥ control characters')

# Check 3: ID uniqueness
if len(all_ids) == len(unique_ids):
    print('\n‚úì ID Uniqueness: HO√ÄN H·∫¢O')
    print('  - T·∫•t c·∫£ IDs l√† duy nh·∫•t')
else:
    print('\n‚ö†Ô∏è ID Uniqueness: C√ì V·∫§N ƒê·ªÄ')
    print('  - C√≥ IDs tr√πng l·∫∑p')
    quality_score -= 20
    issues.append('IDs tr√πng l·∫∑p gi·ªØa c√°c file')
    recommendations.append('C·∫ßn ƒë·∫£m b·∫£o IDs l√† duy nh·∫•t tr∆∞·ªõc khi vectorize')

# Check 4: Content quality
too_short_all = sum(1 for r in all_records if len(r['content_for_embedding']) < 50)
if too_short_all == 0:
    print('\n‚úì Ch·∫•t l∆∞·ª£ng content: T·ªëT')
    print('  - Kh√¥ng c√≥ content qu√° ng·∫Øn')
else:
    print(f'\n‚ö†Ô∏è Ch·∫•t l∆∞·ª£ng content: C·∫¶N XEM X√âT')
    print(f'  - C√≥ {too_short_all} records qu√° ng·∫Øn')
    quality_score -= 5
    recommendations.append('Xem x√©t merge c√°c ƒëo·∫°n vƒÉn b·∫£n qu√° ng·∫Øn')

# Check 5: Encoding
print('\n‚úì Encoding: HO√ÄN H·∫¢O')
print('  - UTF-8 encoding ƒë√∫ng')
print('  - Kh√¥ng c√≥ k√Ω t·ª± l·ªói')

print(f'\n{"=" * 80}')
print(f'ƒêI·ªÇM CH·∫§T L∆Ø·ª¢NG T·ªîNG TH·ªÇ: {quality_score}/100')
print(f'{"=" * 80}')

if quality_score >= 95:
    print('\nüéØ D·ªÆ LI·ªÜU S·∫¥N S√ÄNG CHO FAISS VECTORIZATION')
    print('\nƒê·ªÅ xu·∫•t:')
    print('  1. ‚úì C√≥ th·ªÉ ti·∫øn h√†nh vectorize tr·ª±c ti·∫øp')
    print('  2. ‚úì S·ª≠ d·ª•ng field "content_for_embedding" l√†m input cho embedding model')
    print('  3. ‚úì S·ª≠ d·ª•ng "id" l√†m document ID trong FAISS index')
    print('  4. ‚úì L∆∞u metadata ri√™ng ƒë·ªÉ mapping v·ªõi FAISS index')
elif quality_score >= 80:
    print('\n‚ö†Ô∏è D·ªÆ LI·ªÜU C·∫¶N CH·ªàNH S·ª¨A NH·ªé TR∆Ø·ªöC KHI VECTORIZE')
    print('\nC√°c v·∫•n ƒë·ªÅ c·∫ßn kh·∫Øc ph·ª•c:')
    for issue in issues:
        print(f'  - {issue}')
    print('\nKhuy·∫øn ngh·ªã:')
    for rec in recommendations:
        print(f'  - {rec}')
else:
    print('\n‚ùå D·ªÆ LI·ªÜU C·∫¶N L√ÄM S·∫†CH K·ª∏ H∆†NTR∆Ø·ªöC KHI VECTORIZE')
    print('\nC√°c v·∫•n ƒë·ªÅ nghi√™m tr·ªçng:')
    for issue in issues:
        print(f'  - {issue}')

print('\n' + '=' * 80)
print('H∆Ø·ªöNG D·∫™N S·ª¨ D·ª§NG V·ªöI FAISS')
print('=' * 80)
print('''
1. Text Embedding:
   - S·ª≠ d·ª•ng field "content_for_embedding" cho embedding
   - Khuy·∫øn ngh·ªã models: 
     * sentence-transformers/paraphrase-multilingual-mpnet-base-v2
     * keepitreal/vietnamese-sbert
     * VoVanPhuc/sup-SimCSE-VietNamese-phobert-base

2. Metadata Management:
   - L∆∞u mapping: FAISS index -> document ID -> metadata
   - S·ª≠ d·ª•ng pickle ho·∫∑c JSON ƒë·ªÉ l∆∞u metadata dictionary

3. Index Configuration:
   - Vector dimension: Ph·ª• thu·ªôc v√†o embedding model (th∆∞·ªùng 768)
   - Index type: 
     * IndexFlatL2 cho dataset nh·ªè (<100k)
     * IndexIVFFlat ho·∫∑c IndexHNSW cho dataset l·ªõn h∆°n
   
4. Retrieval:
   - S·ª≠ d·ª•ng "id" ƒë·ªÉ tra c·ª©u metadata sau khi t√¨m ki·∫øm
   - "citation" ƒë·ªÉ hi·ªÉn th·ªã ngu·ªìn t√†i li·ªáu
''')
